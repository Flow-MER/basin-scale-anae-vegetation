{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basin Scale Vegetation Vulnerability\n",
    "\n",
    "Modified from the MDBA BWS Vulnerabilities Project\n",
    "\n",
    "## Data Inputs\n",
    "\n",
    " 1. Australian National Aquatic Ecosystem (ANAE) mapping  v3 - The ANAE identifies different vegetation types and provides the spatial units used to summarise other data. Polygons < 1 Ha are removed as they are too small to meet the reliability requirements of the WIT tool and MODIS derived NDVI.  \n",
    " 1. Geosciences Australia Wetland Insights Tool (WIT) - WIT data observations for all ANAE polygons > 1 Ha in the MDB 1986-present.  Raw data supplied by Geosciences Australia for individual observation dates through the Landsat Record summarised into daily, yearly, all-time and inundation event statistics (a separate jupyter notebook)\n",
    " 1. Normalized Difference Vegetation Index (NDVI) - Average NDVI per ANAE polygon per year 1986-present calculated using google earth engine reducer: shared code: <https://github.com/Flow-MER/GoogleEarthEngine_scripts>\n",
    " 1. Root Zone Soil Moisture (Australian Water Outlook) - Mean root zone soil moisture per ANAE polygon per year was generated using ArcGIS but there are many ways to calculate the annual average per polygon from the AWO netcdf   <https://awo.bom.gov.au/products/historical/soilMoisture-rootZone>\n",
    " 1. Stress thresholds for vegetation based on durations since last inundation for different functional groups that were identified by experts are coded directly into this Jupyter Notebook\n",
    "\n",
    "## Data Outputs\n",
    "\n",
    "This notebook writes the various metric to the working directory in tabular format csv files (spatial units in rows, years in columns) that can be read by Microsoft Excel.  Baseline values and scores are added to the tables as additional columns. There are a **lot** of output files included for spatial scales that were not included in the project report but may be useful for other investigations or to inform water planning at those locations (e.g. DIWA and Ramsar sites)\n",
    "\n",
    "Output files for habitat metrics follow the naming convention: {metric}_{aggregator}_{year_window_width}yr_condition.csv\n",
    "e.g.  pv_median_DIWA_5yr_condition.csv  is the median \"pv\" (green fractional cover) with ANAE polygons aggregated to larger DIWA wetland scales using a 5-year moving window in which to calculate rates of change.  \n",
    "\n",
    "*NOTE:  The outputs generated from this notebook will vary from the report because this code has removed the MDBA Stand Condition tool inputs and made improvements to the NDVI inputs\n",
    "\n",
    "### Mapping the outputs\n",
    "\n",
    "* Patterns can be visualised in GIS by joining the output files to the relevant spatial layers.  Many of the vegetation maps in the report used the ANAE polygons scale to visualise the patterns - this was done by joining **FINAL_BWSVulnerability_vegetation_ANAE.csv** to the **ANAEv3** using the **UID** polygon identifier.  Mapping whole Valley aggregated scores would be done by joining **FINAL_BWSVulnerability_vegetation_Valley.csv** to **BWSRegions.shp** using the **BWS_Region**.\n",
    "\n",
    "## Processing Environment\n",
    "\n",
    "For the project the analysis was conducted in the python processing environment of ArcGIS Pro 3.0 but were coded to use common open source python data processing libraries (Geopandas, Pandas, numpy) that should enable the analysis to be repeated in most environments.\n",
    "Note the code produces many more tables of output data than are required because it calculates parameters for multiple spatial scales.  The BWS Vulnerability Project reports mainly on vegetation outcomes at the scale of MDB Valleys (BWS Vegetation Regions). Outputs are also generated for Ramsar sites, MDBA Waterbird Areas (the \"dirty thirty\" polygons), and individual ANAE polygons.\n",
    "\n",
    "## Repeating or extending the analysis to additional years of data\n",
    "\n",
    "Extending the analysis requires:\n",
    "\n",
    "1. collating new data and appending to the current 1986-2024 source files\n",
    "1. edit the definition of the **alltime** variable to extend past 2024.\n",
    "1. re-run the notebook\n",
    "\n",
    "Source data comes from a variety of places and requires a different technologies to assemble as outlined above.  The current source files should be used as the template to append to,  which should ensure the updated files will run with this workbook.  There is some additional code built into the workbook to re-build spatial relationships among data\n",
    "\n",
    "The code was built to test the method within the confines of a project so it isn't always pretty.    If the logic is not clear please refer to the report and reach out to the report authors with questions.\n",
    "\n",
    "***\n",
    "\n",
    "# Contact\n",
    "\n",
    "* Dr Shane Brooks\n",
    "\n",
    "* <https://brooks.eco>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages\n",
    "Import Python packages that are used for the analysis.\n",
    "\n",
    "Use standard import commands; some are shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append(\"D:\\Tools\\Python\\arcgispro-py3-clone-new\\Library\\bin\")\n",
    "#os.environ['GDAL_DATA'] = 'C:/Program Files/ArcGIS/Pro/Resources/pedata/gdaldata'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from tqdm.notebook import tqdm\n",
    "from osgeo import gdal\n",
    "\n",
    "#allow plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# User Defined Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the path to the spatial data (shape files)\n",
    "spatial_path = 'D:/CSIRO_Ramsar_Climate_Change_Vulnerability/data/spatial'\n",
    "\n",
    "\n",
    "#set the path to the data input files\n",
    "data_path = 'D:/CSIRO_Ramsar_Climate_Change_Vulnerability/data/input'\n",
    "\n",
    "#  set the working directory\n",
    "working_directory = 'D:/CSIRO_Ramsar_Climate_Change_Vulnerability/data/output'\n",
    "\n",
    "\n",
    "# change to the user specified working directory so the worker gets written to the correct location\n",
    "os.chdir(working_directory)\n",
    "cwdpath = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise data stores  (names are data frames used in the code)\n",
    "# this allows cells in the notebook to be re-run quickly without re-reading all the iput data multiple times when we dont have to\n",
    "\n",
    "ANAE = None\n",
    "DTwaterbirds = None\n",
    "DIWA = None\n",
    "Valley = None\n",
    "Ramsar = None\n",
    "wb = None\n",
    "analogue = None\n",
    "\n",
    "wit_yearly = None\n",
    "tsli_master = None\n",
    "idf_master = None\n",
    "ndvi = None\n",
    "soilmoisture_df = None\n",
    "TSC_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define year ranges used by the method\n",
    "\n",
    "alltime = list(range(1987, 2021)) #2021 was the last year with complete data  1986 is excluded as it has incomplete data\n",
    "millennium_drought = list(range(2001, 2010))  #excludes 2010 because last year in a range is not included\n",
    "#yearcols is a dictionary to translate numberic year column totals of pivot tables into strings as required for shapefiles and csv headers\n",
    "yearcols = {y: 'y'+str(y) for y in alltime}\n",
    "\n",
    "#VEGETATION\n",
    "veg_window_width = 5  # veg condition/stress is averaged over a moving 5 year window up to a given year\n",
    "veg_trend_width = 2   # veg metric trends are measured in the most recent 2 years leading up to a given year\n",
    "\n",
    "\n",
    "\n",
    "#Vegetation thresholds from Cherie Campbell noting that some ranges are imprecise and have gaps.  \n",
    "\n",
    "#the three bins are defined in python using three numbers [0. threshold#1, threshold2]\n",
    "\n",
    "#and scored as:  0 > LOW > threshold#1 > MEDIUM > threshold#2 > HIGH \n",
    "\n",
    "\n",
    "#Vegetation stress thresholds based on the time-since-last-inundation (tsli)\n",
    "#Threshold scores are set for three bins (LOW, MEDIUM and HIGH stress)\n",
    "vegetation_tsli_stress_thresholds = {\n",
    "    'river red gum swamps and forests': [0, 730, 1825], # RRG swamps forests and woodlands 1-2 years, 3-4 years, ≥ 5 years\n",
    "    'river red gum woodland': [0, 1460, 2555], # RRG swamps forests and woodlands 1-2 years, 5 years, ≥ 7 years\n",
    "    'black box': [0, 1460, 2555], # Black box, 3 – 4 years, 5 – 6 years, ≥ 7 years\n",
    "    'cooba': [0, 1460, 2555], # 1-2 years, 5 years, ≥ 7 years\n",
    "    'coolibah': [0, 3650, 7300], # Coolibah, 10 years, 20 years, > 20 years\n",
    "    'lignum': [0, 1095, 2555], # Lignum, 3 years, 4 years, ≥ 7 years\n",
    "    'shrubland': [0, 1095, 3650], # chenopods/shrubland, 3 years, 4 years, ≥ 10 years\n",
    "    'submerged lake': [0, 90, 120], # Submerged vegetation, < 3 months, 3 – 4 months, > 4 months\n",
    "    'tall reed beds': [0, 365, 730 ], # Tall reeds, < 1 year, 1 – 2 years, > 2 years\n",
    "    'grassy meadows': [0, 240, 300], # Grassy meadows, < 8 months, 8 – 10 months, > 10 months\n",
    "    'herbfield': [0, 365, 1460], # Herb fields, 1 year, 2 – 4 years, > 4 years\n",
    "    'clay pan': [0, 3650, 7300] #not vegetated but used for waterbirds  10 years, 20 years, > 20 years\n",
    "}\n",
    "\n",
    "# Waterbird stress metric based on the time-since-last-inundation (tsli) of wetland vegetation the waterbirds depend on\n",
    "# Threshold scores are set for three bins (LOW, MEDIUM and HIGH stress)\n",
    "# the alignment of waterbirds to habitat types is explained in the roport and is defined in the dictionary below waterbird_dom_habitats\n",
    "waterbird_habitat_stress_thresholds = {\n",
    "     'river red gum swamps and forests': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'river red gum woodland': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'black box': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'cooba': [0, 1460, 2555], # 1-2 years, 5 years, ≥ 7 years\n",
    "     'shrubland': [0, 1095, 3650], # chenopods/shrubland, 3 years, 4 years, ≥ 10 years\n",
    "     'coolibah': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'lignum': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'submerged lake': [0, 365, 1095], # <1 year, 1-3 years, ≥ 3 years\n",
    "     'tall reed beds': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'grassy meadows': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'herbfield': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "     'clay pan': [0, 365, 1460], # <1 year, 1-4 years, ≥ 5 years\n",
    "}\n",
    "\n",
    "# Define a dictionary that identifies the importnat dominant ecosytem type groups for different\n",
    "# waterbird functional groups. This is used to take vegetation/habitat metrics to waterbird groups\n",
    "    \n",
    "waterbird_dom_habitats = {\n",
    "    'Aerial_divers': ['submerged lake'],\n",
    "    'Colonial_nesters':['river red gum swamps and forests'],\n",
    "    'Cryptic_waders': ['herbfield', 'tall reed beds'],\n",
    "    'Diving_swimmers': ['river red gum swamps and forests'],\n",
    "    'Filtering': ['river red gum swamps and forests'],\n",
    "    'Grazing_swimmers': ['river red gum swamps and forests'],\n",
    "    'Shorebirds': ['submerged lake']\n",
    "}   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def pivot_year(df, wit_metric, pkey='UID'):\n",
    "    \"\"\"\n",
    "        Pivot the input data to a dataframe with years as column headers\n",
    "        calculate the mean and stddev for the baseline years =1989-2022 excluding the millinium drought\n",
    "        \n",
    "        count =  number of years with waterbird counts\n",
    "        baseline = mean of baseline years\n",
    "        max = maximum value for baseline period\n",
    "        median = median value for baseline period\n",
    "        mad =  median absolute deviation baseline\n",
    "               mad is a non-parametric standard deviation used with the\n",
    "               waterbird data because there are lots of spatial units with no or few count records\n",
    "    '''\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    allyears = df['year'].unique().tolist()\n",
    "    baseline = [y for y in allyears if y not in millennium_drought]   #alltime excluding the millennium_drought\n",
    "    \n",
    "    \n",
    "    pivot = df.pivot(index = pkey, columns='year', values=wit_metric )\n",
    "        #count of how many years have data\n",
    "    pivot['count'] = pivot.count(axis=1, numeric_only=True)\n",
    "\n",
    "    #baseline = average metric for baseline years \n",
    "    pivot['baseline'] = pivot[baseline].mean(axis=1, numeric_only=True)\n",
    "\n",
    "    #stddev = stdev in metric for baseline years\n",
    "    pivot['stddev'] = pivot[baseline].std(axis=1, numeric_only=True)\n",
    "    #pivot['dev'+str(recent[-1])] = ((pivot[recent[-1]]-pivot['baseline'])/pivot['stddev'])+0\n",
    "    \n",
    "    #baseline = average metric for baseline years \n",
    "    pivot['max'] = pivot[baseline].max(axis=1, numeric_only=True)\n",
    "    \n",
    "    \n",
    "    pivot['median'] = pivot[baseline].median(axis=1, numeric_only=True)\n",
    "\n",
    "    tmp=pd.DataFrame()\n",
    "    for y in baseline:\n",
    "        tmp[y] = abs(pivot['median'] - pivot[y])\n",
    "    #tmp.to_csv(\"DEBUG_tmp.csv\")\n",
    "    pivot['mad'] = tmp.median(axis=1, numeric_only=True)\n",
    "\n",
    "    #deviation from baseline in each of the recent (last 5 years) standardised by the stddev\n",
    "    #for y in baseline[-5:]:\n",
    "    #    pivot['cond'+str(y)] = (pivot[y]-pivot['baseline'])/pivot['stddev']  #+0 converts -0 to 0\n",
    "\n",
    "    #sum negative scores in recent (last 5)    \n",
    "    #ya = ['status'+str(y) for y in recent[::-1]]\n",
    "    #pivot['sum5y_neg'] = pivot[ya][pivot[ya] < 0].sum(axis=1, numeric_only=True)\n",
    "    return pivot\n",
    "\n",
    "def fn_slope(d):\n",
    "    \"\"\"\n",
    "        calculate the rate of change as the slope through the supplied points\n",
    "        input is a series of values\n",
    "        output is the rate of change\n",
    "    \"\"\"\n",
    "    yvalues = d.values\n",
    "    if len(yvalues) < 2:\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        xvalues = list(range(0,len(yvalues)))\n",
    "        #standardise xvalues to keeps spread of slope roughly -1 to 1  as number of x values varies\n",
    "        xmean=np.mean(xvalues)\n",
    "        xstdev=np.std(xvalues)\n",
    "        standardised_xvalues = [(x-xmean)/xstdev for x in xvalues]\n",
    "        return np.polyfit(standardised_xvalues, yvalues, 1)[0].round(4) #slope\n",
    "\n",
    "def fn_average_trend (df, period, trend_period = None, nobaseline = False):\n",
    "    \"\"\"\n",
    "        calculate the average rate of change within a windows of x years (the trend_period)\n",
    "    \"\"\"\n",
    "    if trend_period is None:\n",
    "        trend_period = period\n",
    "    elif len(trend_period) > len(period):\n",
    "        raise Exception(\"trend_window_width must be less than or equal to year_window_width\")\n",
    "    #calculate mean and slope of the 5years values difference from baseline standardised by the stddev\n",
    "\n",
    "    if nobaseline:\n",
    "        tmp = df[period]\n",
    "    else:\n",
    "        tmp = df[period].subtract(df['baseline'], axis=0).div(df['stddev'], axis=0)+0\n",
    "    #tmp=tmp.dropna()\n",
    "    tmp=tmp.fillna(0) # sites with no variation (eg always dry have baseline=0, stddev=0 so standardised metric becomes na from divide by zero - recast to zero\n",
    "    #debug print ('period', period)\n",
    "    slope = 'Trend'+ str(period[-1])\n",
    "    ave = 'Ave' + str(period[-1]) #capital A ensures column sorts first\n",
    "    tmp[ave] = tmp[period].mean(axis=1, numeric_only=True)\n",
    "    cols = [ave]\n",
    "    if len(period) > 1:\n",
    "        #print ('trend_period', trend_period)\n",
    "        #print(tmp)\n",
    "        tmp[slope] = tmp[trend_period].apply(fn_slope, axis=1)\n",
    "        cols.append(slope)\n",
    "    #debug tmp.to_csv(str(year)+\"fn_average_trend_tmp.csv\")\n",
    "    return tmp[cols]\n",
    "\n",
    "# def aggregate_fields (ANAE, aggshp, aggfield):\n",
    "#     agg = gpd.read_file(aggshp).to_crs(\"EPSG:3577\")[aggfield + ['geometry']]\n",
    "#     grp_labels = gpd.sjoin(ANAE, agg, how=\"left\", op='intersects')[['UID', 'ANAE_TYPE','Area_Ha'] + aggfield].set_index('UID')\n",
    "#     return grp_labels, aggfield\n",
    "\n",
    "#def aggregate_area_weighted(df, agg, aggfield, ANAE = ANAE, ANAEgrp = [], pkey='UID'):\n",
    "def aggregate_area_weighted(df, agg, aggfield, ANAEgrp = []):\n",
    "    \"\"\"\n",
    "        Takes parameter values for individual ANAE ecosystem polygons and aggregates the values to larger areas\n",
    "        using an area weighting.  e.g. to aggregate a metric across all the ANAE polygons within a Ramsar site\n",
    "        inputs: dataframe of parameter values per ANAE polygon\n",
    "                a specified aggregator (one or more larger subunits that contain multiple ANAE polygons)\n",
    "        \n",
    "        Output is a single metric value for each larger area subunit calcualted the area weighted mean of the ANAE polygons within it\n",
    "    \"\"\"\n",
    "    cols = df.columns.values.tolist()\n",
    "    #debug df.to_csv(\"fn_aggregate_area_weighted_df.csv\")\n",
    "    #debug grp_labels.to_csv(\"fn_aggregate_area_weighted_grp_labels.csv\")\n",
    "\n",
    "    agdf = df.join(agg, how='inner')\n",
    "    agdf = agdf.replace([np.inf, -np.inf], np.nan)  #there are some stray \"inf\" values from dividing by very small small stddev -covert to NaN so sum(numeric_only = True) can ignore them\n",
    "\n",
    "    #debug agdf.to_csv(\"fn_aggregate_area_weighted_agdf.csv\")\n",
    "    if agg.name == 'ANAE':\n",
    "        return agdf[['grp'] + cols +['Area_Ha']].set_index('grp', append=True)\n",
    "    else:\n",
    "        agdf[cols] = agdf[cols].multiply(agdf['Area_Ha'], axis = 0)\n",
    "        #debug agdf.to_csv(\"fn_aggregate_area_weighted_agdftimesArea.csv\")\n",
    "        agg_data = agdf[cols +['Area_Ha']+aggfield+ANAEgrp].groupby(aggfield+ANAEgrp).sum(numeric_only = True)\n",
    "        #debug agg_data.to_csv(\"fn_aggregate_area_weighted_agg_data.csv\")\n",
    "        agg_data[cols] = agg_data[cols].div(agg_data['Area_Ha'], axis = 0)\n",
    "        return agg_data[cols+['Area_Ha']]\n",
    "\n",
    "def bin_stress_scores(x, params):\n",
    "    (thresholds, colname) = params\n",
    "    '''\n",
    "    bin values into 3,2,1 (low, medium high) = reverse of condition binning \n",
    "    applying pre-determined thresholds mapped in tsl_score dict.\n",
    "    '''\n",
    "    _bins = thresholds[x['grp'].iat[0]]+[float(\"inf\")]\n",
    "    #debug print (x['grp'].iat[0],_bins,list(range(len(_bins)-1,0,-1)))\n",
    "    return pd.cut(x[colname], bins=_bins, right=False, labels=range(len(_bins)-1,0,-1)).astype('float') #return as float instead of category so we can multiply by area to scale up\n",
    "\n",
    "def rename_stats_columns(c, metric_name):\n",
    "    '''\n",
    "        a clumsy routine in ever evolving code to remname column headers in the data frame\n",
    "    '''\n",
    "    c = c.replace('Ave',metric_name)\n",
    "    c = c.replace('Trend','T'+metric_name)\n",
    "    return c\n",
    "\n",
    "def deviation_from_baseline(df, metric, year_window_width, trend_window_width, nobaseline = False):\n",
    "    '''\n",
    "        calculate the deviation from the baseline in each year of the data frame\n",
    "        \n",
    "        append also the Trend in the deviations over the trend_window_width with prefix \"T\" on columns headings\n",
    "        \n",
    "        append also the SUM deviation over the year_window_width with prefix \"sum\" on columns headings (e.g. sum of the preceeding 5 years)\n",
    "    '''\n",
    "    allyears = df['year'].unique().tolist()\n",
    "    baseline = [y for y in allyears if y not in millennium_drought]   #alltime excluding the millennium_drought\n",
    "    _years=range(allyears[0]+year_window_width-1,allyears[-1]+1)\n",
    "    metric_name = metric.replace('+','').lower()\n",
    "    \n",
    "    pivot = pivot_year(df, metric, pkey).round(4)\n",
    "    pivot.to_csv(f\"BWS_pivot_{metric}.csv\")\n",
    "    dfs = []\n",
    "    for p, year in enumerate(tqdm(_years, desc = f\"{metric} in {year_window_width}y window, trend over {trend_window_width}y:\")):\n",
    "        period = list(range(year-year_window_width+1,year+1))\n",
    "        trend_period = period[-trend_window_width:]\n",
    "        stats_df = fn_average_trend(pivot, period, trend_period, nobaseline = nobaseline)\n",
    "        #debug stats_df.to_csv(str(year)+\"testmetrics.csv\")\n",
    "        #debug print(year,stats_df)\n",
    "        ave = stats_df.columns.values.tolist()[0]\n",
    "        #if we want to score before aggregating\n",
    "        #stats_df['sc'+metric+str(year)] = pd.cut(stats_df[ave], bins = _bins, labels = range(1,len(_bins))).astype('float') #float not default category so can be aggregated\n",
    "        if len(stats_df.columns) > 1:\n",
    "            trend = stats_df.columns.values.tolist()[1]\n",
    "            stats_df['sum'+metric_name+str(year)] = stats_df.sum(axis=1)\n",
    "            #if we want to score before aggregating\n",
    "            #stats_df['scT'+metric+str(year)] = pd.cut(stats_df[ave], bins = _bins, labels = range(1,len(_bins))).astype('float') #float not default category so can be aggregated           \n",
    "        dfs.append(stats_df)\n",
    "    #aggregate all the metrics into a single data frame\n",
    "    metrics_df = pd.concat(dfs, axis=1).sort_index(axis=1)\n",
    "    metrics_df = metrics_df.rename(columns = {c: rename_stats_columns(c.strip(), metric_name) for c in metrics_df.columns})\n",
    "    return metrics_df\n",
    "\n",
    "def append_metric_scores(df, col_list, _bins, _labels):\n",
    "    '''\n",
    "        scores the input data frame into bins and appends the scores for each year as\n",
    "        additonal columns added to the right edge of the data frame so the data can be easily viewed in Excel\n",
    "    \n",
    "    '''\n",
    "    for col in col_list:\n",
    "        df['sc'+col] = pd.cut( df[col], bins = _bins, labels = _labels, include_lowest = True).astype('float') #defaults to 'category' so recast to float so scores can be aggregated\n",
    "    return df\n",
    "\n",
    "def calc_pivots (df, metrics, year_window_width=5, trend_window_width=None, _bins=[float('-inf'),-1,0,float('inf')], reverse_scores=False, nobaseline=False, tag=''):\n",
    "    '''\n",
    "        This brings together some of the code above to\n",
    "        summarise the metrics in a moving window of multiple years\n",
    "        (for the BWS vulnerabilities project the veg condition and trend (rate of change) over a\n",
    "        moving 5 years period was calculated for each year of the data frame\n",
    "        \n",
    "        The metrics are then aggregated to the pre-defined larger spatial subunits (Ramsar sites, waterbird breeding sites, valleys)\n",
    "        \n",
    "        Pivot tables are written to the working directoy as cvs files for inspection in Excel.\n",
    "        The pivot tables will also be read in and scored for the final integration of vulnerability metrics\n",
    "        \n",
    "        reverse_scores true/false is used to switch the logic for different metrics \n",
    "        e.g. more green veg (WIT pv) is good, more bare soil (WIT bs) is bad\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    if trend_window_width is None:\n",
    "        trend_window_width = year_window_width\n",
    "    elif trend_window_width > year_window_width:\n",
    "        raise Exception(\"trend_window_width must be less than or equal to year_window_width\")\n",
    "    _scores = list(range(1,len(_bins)))\n",
    "    if reverse_scores: _scores=_scores[::-1]\n",
    "    wit={}\n",
    "    for metric in metrics:\n",
    "        metrics_df = deviation_from_baseline(df, metric, year_window_width, trend_window_width, nobaseline = nobaseline)\n",
    "        print('Aggregate metrics and scores:')\n",
    "        for ag in aggregators:\n",
    "            fname = f\"{metric}_{ag}_{year_window_width}yr{tag}.csv\"\n",
    "            print (f\"     {ag} - {fname}\")\n",
    "            wit[ag] = aggregate_area_weighted(metrics_df, aggregators[ag], aggfield[ag], ANAEgrp = ['grp']).round(4)\n",
    "            col_list = [c for c in wit[ag].columns.values.tolist() if c != 'Area_Ha']\n",
    "            wit[ag] = append_metric_scores(wit[ag], col_list, _bins, _scores)\n",
    "            wit[ag].to_csv(fname)\n",
    "\n",
    "def standardise (df):\n",
    "    '''\n",
    "        Standardises an input data frame to values between 0-1\n",
    "        used to standardise NDVI from older NOAA AVHRR and newer MODIS\n",
    "    '''\n",
    "    dmin = df.min()\n",
    "    drange = df.max() - dmin\n",
    "    return df.subtract(dmin).divide(drange)\n",
    "\n",
    "\n",
    "def standardise_z (df):\n",
    "    '''\n",
    "        z-score standardise an input data frame \n",
    "        used to standardise NDVI from older NOAA AVHRR and newer MODIS\n",
    "    '''\n",
    "    dmean = df.mean()\n",
    "    dstdev = df.std()\n",
    "    return df.subtract(dmean).divide(dstdev)\n",
    "\n",
    "\n",
    "def infill_years(series):\n",
    "    '''\n",
    "      used to fill out a series of years when some years are not represented due to missing data\n",
    "      i.e. wehen there are no waterbird counts for a year in a location\n",
    "    '''\n",
    "    indexnames = list(series.index.names)\n",
    "    #notyear = [x for x in indexnames if x != 'year']\n",
    "    notyear = ','.join(x for x in indexnames if x != 'year')\n",
    "    df=pd.DataFrame(series)\n",
    "\n",
    "    #fill years without breeding with 0\n",
    "    mux = pd.MultiIndex.from_product([\n",
    "            fullperiod,\n",
    "            df.reset_index()[notyear].unique()\n",
    "            ], names=indexnames)\n",
    "    return df.reindex(mux).fillna(0)\n",
    "\n",
    "\n",
    "def get_nearest_distance(left, right, search_distance, name, label):\n",
    "    \"\"\"get distance between to features (passed as left and right)\"\"\"\n",
    "    search_area = left.buffer(search_distance)\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(search_area)):\n",
    "        geom = search_area.geometry.iloc[i]\n",
    "        query = right.sindex.query(geom)\n",
    "        dist = right.iloc[query].distance(left.geometry.iloc[i])\n",
    "        min_dist = dist.min()\n",
    "        if 0 <= min_dist <= search_distance:\n",
    "            right_idx = dist.idxmin()\n",
    "            row = [right_idx, right.iloc[right_idx][label], min_dist]\n",
    "        else:\n",
    "            #no features within search distance\n",
    "            row = [-1, float('NaN'), -1]\n",
    "        data.append(row)\n",
    "    return pd.DataFrame(data, index = left.index, columns=[name + '_idx', name + '_name', name + '_dist'])\n",
    "\n",
    "def waterbird_metrics(df, aggregator, breeding=''):\n",
    "    '''\n",
    "    summarise various waterbird counts\n",
    "    Annual waterbird totals are estimated as the larger number of:\n",
    "        * the maximum count per spatial unit from aerial survey; or\n",
    "        * the combined aggregate of individual observations\n",
    "        \n",
    "    The logic here is that you cant just sum waterbird records because \n",
    "    individual ALA obvservations are counting the same individuals as seen in the aerial surveys\n",
    "    also...  ALA records can also be multiple observations of the same individuals\n",
    "    \n",
    "    '''\n",
    "\n",
    "    big_surveys = ['Murray Icon', 'SEA', 'CLLMM', 'Eastern Australian Survey' , 'Hydrological Indicator Sites']\n",
    "    indivperANAE = df[~df.cCode.isin(big_surveys)].groupby(['year','ANAE_idx', aggregator, 'grp', 'vName'])['iCount'].max()\n",
    "    indivperaggregate = indivperANAE.groupby(['year', aggregator, 'grp', 'vName']).sum()\n",
    "    each_big_surveys_peraggregate = df[df.cCode.isin(big_surveys)].groupby(['year', 'cCode', aggregator, 'grp', 'vName'])['iCount'].sum()\n",
    "    max_big_surveys_peraggregate = each_big_surveys_peraggregate.groupby(['year', aggregator, 'grp', 'vName']).max()\n",
    "\n",
    "    indiv = pd.concat([indivperaggregate,max_big_surveys_peraggregate]).groupby(['year', aggregator, 'grp', 'vName']).max()\n",
    "    grpindiv = indiv.groupby(['year', aggregator, 'grp']).sum()\n",
    "    \n",
    "    sr = df.groupby(['year', aggregator])['vName'].nunique()\n",
    "    grpsr = df.groupby(['year', aggregator, 'grp'])['vName'].nunique()\n",
    "\n",
    "    indiv.to_csv('indivcount'+aggregator+breeding+'.csv')\n",
    "    grpindiv.to_csv('grpindiv'+aggregator+breeding+'.csv')\n",
    "    return indiv, grpindiv, sr, grpsr\n",
    "\n",
    "\n",
    "def extract_scores (ag, fname, score_field_name):\n",
    "    '''\n",
    "        Retrieves the score columns from specified output pivot tables\n",
    "        and strips off the string prefix from year columns.\n",
    "        This standardises the format of the different score tables so metrics\n",
    "        can be summed and counted across groups and features\n",
    "        with pandas append and groupby functions        \n",
    "    '''\n",
    "    #print(f'     Reading scores from {fname}...') #DEBUG\n",
    "    scores = pd.read_csv(fname, low_memory=False)\n",
    "    colnames = [f\"{score_field_name}{y}\" for y in years] \n",
    "    scores = scores[aggfield[ag]+['grp']+colnames].set_index(aggfield[ag]+['grp'])\n",
    "    return scores.rename(columns = {c: c[-4:] for c in scores.columns})\n",
    "\n",
    "def habitat_to_waterbird_groups (ag, score_df):\n",
    "    wb_grp_scores = []\n",
    "    idx =score_df.index.names\n",
    "    #summed aggregate area per spatial unit needed to to area weighting if a waterbird group is spread across more than one habitat types (Cryptic waders)\n",
    "    area = aggregators[ag][['Area_Ha']+['grp']+aggfield[ag]].groupby(aggfield[ag]+['grp']).sum()\n",
    "    #print (area) #DEBUG\n",
    "    for wb_group in waterbird_dom_habitats:\n",
    "        \n",
    "        #get habitat rows from stress_df that correspond to the waterbird group\n",
    "        grp_df = score_df.iloc[score_df.index.get_level_values('grp').isin(waterbird_dom_habitats[wb_group])].copy()\n",
    "        cols = grp_df.columns.tolist()\n",
    "        grp_df =  grp_df.join(area)\n",
    "        grp_df = grp_df.reset_index()  #removing the index makes it easy to rename the habitats to the waterbird groups\n",
    "        #rewrite all the selected habitat types to the waterbird group name\n",
    "        grp_df['grp'] = wb_group\n",
    "        if len(waterbird_dom_habitats[wb_group]) > 1:\n",
    "            # combine scores for multiple habitats using area-weighting\n",
    "            #  print (f'          {waterbird_dom_habitats[wb_group]} assigned to {wb_group} - area weighted average') #DEBUG\n",
    "            grp_df[cols] = grp_df[cols].multiply(grp_df['Area_Ha'], axis = 0)\n",
    "            agg_data = grp_df[cols +['Area_Ha']+idx].groupby(idx).sum(numeric_only = True)\n",
    "            agg_data[cols] = agg_data[cols].div(agg_data['Area_Ha'], axis = 0)\n",
    "            grp_df = agg_data[cols]  #removes the Area_Ha so it doesnt interfere with rescaling the frames to 0-1 later\n",
    "\n",
    "        else:\n",
    "            #print (f'          {waterbird_dom_habitats[wb_group]} assigned to {wb_group}') #DEBUG\n",
    "            grp_df = grp_df.set_index(idx).drop('Area_Ha', axis=1) #removes the Area_Ha so it doesnt interfere with rescaling the frames to 0-1 later\n",
    "        wb_grp_scores.append(grp_df)\n",
    "        #print (grp_df)  #DEBUG\n",
    "    wb_scores = pd.concat(wb_grp_scores)\n",
    "    return wb_scores\n",
    "\n",
    "def normalise_data (df):\n",
    "    df_min = min(df.min())\n",
    "    df_max = max(df.max())\n",
    "    return df.subtract(df_min).divide(df_max-df_min)\n",
    "\n",
    "def sum_and_normalise_data_weighted (df, max_metric_count):\n",
    "    '''\n",
    "        sums the condition/stress scores for each feature in the index and rescales the data\n",
    "        normalising to range 0-1 allowing for cells with missising data\n",
    "        rescaled = (sum - count) / (count * number of possible metrics) - count)\n",
    "    '''\n",
    "    sum_df = df.groupby(level=df.index.names).sum()\n",
    "    count_df = df.groupby(level=df.index.names).count()\n",
    "    return sum_df.subtract(count_df).divide(count_df.multiply(max_metric_count).subtract(count_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the spatial subunits - ANAE and aggregating layers\n",
    "\n",
    "This block of code is slow to run as it:\n",
    "1. first reads in the ANAE polygons\n",
    "2. spatially join the ANAE to multiple data sets with larger-scale subunits to map the aggregations of individual ANAE polgons required to represent larger areas (e.g. Ramsar sites, DIWA wetlands, Valleys\n",
    "\n",
    "The spatial data is read in once and stored and will not be re-read if the cell is re-run.  If the data needs to be read again either reset the notebook at start again or re-run the cell above that initialises the data stores to NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the unique ID identifying each ANAEv3 polygon is a 9 character geohash \n",
    "pkey = 'UID'\n",
    "\n",
    "#dictionary of the aggregators - for the BWS vulnerabilites project we were interested in scaling up from ANAE polygons to larger subunits including\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#DTwaterbirds =  dirty thirty =  MDBA waterbird units\n",
    "#DIWA = Directory of Important Wetlands \n",
    "#Ramsar\n",
    "#Valleys are valleys used by MDBA uses as \"vegetation regions\" to guide priority setting in the BWS\n",
    "#Basin is the whole MDB\n",
    "\n",
    "#aggfield is the unique identifier for subunits within each of the aggregator data sets\n",
    "# note for Ramsar sites we can aggregate ANAE polygons to multiple \"wetlands\" within each ramsar site\n",
    "aggfield = {\n",
    "    'ANAE': ['UID'],\n",
    "    'DTwaterbirds': ['LABEL'],\n",
    "    'DIWA': ['WNAME'],\n",
    "    'Ramsar': ['RAMSAR_NAM','WETLAND_NA'],\n",
    "    'Valley': ['BWS_Region'],\n",
    "    'Basin' : [],\n",
    "    'analogue': ['analogue'],\n",
    "}\n",
    "\n",
    "#spatial data is read into geopandas data frames  \n",
    "\n",
    "#if we havn't read the ANAE data in before the do it now reading into a geopandas frame\n",
    "#for the ANAE we simplify the typology by grouping some of the ANAE ecosystem types with the same dominant vegetation\n",
    "#e.g. we take black box floodplains and blackbox woodland swamps and combine into a single \"black box\" class\n",
    "  \n",
    "if ANAE is None:\n",
    "    print ('Reading ANAE...')\n",
    "    ANAE = gpd.read_file(os.path.join(spatial_path, 'ANAEv3_BWS.shp')).to_crs(\"EPSG:3577\")\n",
    "    ANAE = ANAE[['UID','ANAE_TYPE','Area_Ha','geometry']]\n",
    "    \n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('river red gum', case=False), 'grp'] = 'river red gum swamps and forests'\n",
    "    ANAE.loc[(ANAE['ANAE_TYPE'].str.contains('river red gum', case=False)) &\n",
    "             (ANAE['ANAE_TYPE'].str.contains('woodland', case=False)), 'grp'] = 'river red gum woodland'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('black box', case=False), 'grp'] = 'black box'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('coolibah', case=False), 'grp'] = 'coolibah'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('lignum', case=False), 'grp'] = 'lignum'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('cooba', case=False), 'grp'] = 'cooba'  ##added for CSIRO climate change\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('F2.4: shrubland riparian zone or floodplain', case=False), 'grp'] = 'shrubland'  ##added for CSIRO climate change\n",
    "    \n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('permanent lake|permanent wetland|aquatic bed', case=False), 'grp'] = 'submerged lake'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('tall emergent marsh', case=False), 'grp'] = 'tall reed beds'\n",
    "    ANAE.loc[(ANAE['ANAE_TYPE'].str.contains('grass|meadow', case=False)), 'grp'] = 'grassy meadows'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('forb marsh|temporary wetland|temporary lake', case=False), 'grp'] = 'herbfield'\n",
    "    ANAE.loc[ANAE['ANAE_TYPE'].str.contains('clay', case=False), 'grp'] = 'clay pan'\n",
    "    #setting the names of the data frames is important to permit lookup of appropriate aggregate fields in aggfield dictionary \n",
    "    ANAE.name = 'ANAE' #used to area-weight aggregate all ANAE polygons\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Load Aggregator shapefiles and use spatial joins to determine the ANAE polygons\n",
    "# that are within each larger spatial unit. In theory we could have saved these\n",
    "# as lookup tables for speedy re-use but left coded this way for possible flexibility in the future\n",
    "# if waterbird boundaries or Ramsar boundaries change\n",
    "\n",
    "\n",
    "# if DTwaterbirds is None:\n",
    "#     print ('Reading Dirty Thirty Waterbird areas...')\n",
    "#     DTwaterbirds = gpd.read_file(os.path.join(spatial_path, 'waterbirds_dirty_thirty.shp')).to_crs(\"EPSG:3577\")\n",
    "#     DTwaterbirds = gpd.sjoin(ANAE, DTwaterbirds[aggfield['DTwaterbirds'] + ['geometry']], how=\"left\", predicate='intersects').dropna().set_index('UID')\n",
    "#     DTwaterbirds.name = 'DTwaterbirds'\n",
    "# if DIWA is None:\n",
    "#     print ('Reading DIWA...')\n",
    "#     DIWA = gpd.read_file(os.path.join(spatial_path, 'DIWA_complex.shp')).to_crs(\"EPSG:3577\")\n",
    "#     DIWA = gpd.sjoin(ANAE, DIWA[aggfield['DIWA'] + ['geometry']], how=\"left\", predicate='intersects').dropna().set_index('UID')\n",
    "#     DIWA.name = 'DIWA'\n",
    "if Ramsar is None:\n",
    "    print ('Reading Ramsar...')\n",
    "    Ramsar = gpd.read_file(os.path.join(spatial_path, 'ramsar_wetlands.shp')).to_crs(\"EPSG:3577\")\n",
    "    Ramsar = gpd.sjoin(ANAE, Ramsar[aggfield['Ramsar'] + ['geometry']], how=\"left\", predicate='intersects').dropna().set_index('UID')\n",
    "    Ramsar.name = 'Ramsar'\n",
    "# if Valley is None:\n",
    "#     print ('Reading BWS valleys...')\n",
    "#     Valley = gpd.read_file(os.path.join(spatial_path, 'BWSRegions.shp')).to_crs(\"EPSG:3577\")\n",
    "#     Valley = gpd.sjoin(ANAE, Valley[aggfield['Valley'] + ['geometry']], how=\"left\", predicate='intersects').dropna().set_index('UID')\n",
    "#     Valley = Valley[Valley['Area_Ha'].notnull()]    \n",
    "#     Valley.name = 'Valley'\n",
    "\n",
    "if analogue is None:\n",
    "    print ('Reading analogue...')\n",
    "    analogue = gpd.read_file(os.path.join(spatial_path, 'analogue.shp')).to_crs(\"EPSG:3577\")\n",
    "    analogue = gpd.sjoin(ANAE, analogue[aggfield['analogue'] + ['geometry']], how=\"left\", predicate='intersects').dropna().set_index('UID')\n",
    "    analogue.name = 'analogue'\n",
    "\n",
    "\n",
    "if ANAE.index.name != 'UID':\n",
    "    ANAE = ANAE.set_index('UID')\n",
    "    ANAE.name = 'ANAE'\n",
    "Basin = ANAE[['grp', 'Area_Ha']]\n",
    "Basin.name = 'Basin'\n",
    "\n",
    "\n",
    "aggregators = {'ANAE': ANAE, 'DTwaterbirds':DTwaterbirds, 'DIWA': DIWA, 'Ramsar': Ramsar, 'Valley': Valley, 'Basin': Basin} \n",
    "\n",
    "\n",
    "aggregators = {'ANAE': ANAE,'Ramsar': Ramsar, 'analogue': analogue,}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregators = {'ANAE': ANAE,'Ramsar': Ramsar, 'analogue': analogue,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a debug check to list the ANAE types that are NOT assigned to a functional group to see if we missed anything obvious\n",
    "# the types that list here are the types we *don't* include in the determination of wetland/floodplain/waterbird vulnerabilities\n",
    "# (e.g. includes rivers and streams, saline systems)\n",
    "\n",
    "ANAE[ANAE['grp'].isna()]['ANAE_TYPE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the WIT annual metrics\n",
    "Read in the WIT yearly metrics and calculate pivot tables (note this is slow process but a progress bar is shown).  After the initial calculation of metrics per ANAE polygon is complete the data are aggregated to the various larger spatial scales.\n",
    "\n",
    "\n",
    "**input:** WIT Yearly statistics generated by the wit_metrics notebook as file **RESULT_ANAE_yearly_metrics.csv**\n",
    "\n",
    "**output:** pivot tables for each WIT metric x spatial aggregator combination as CSV files in the working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wit_yearly is None:   #read the data in if hasn't been read in already\n",
    "    #use geopandas instead of pandas so we can later call on the 'name' attribute whcih is not in the version of pandas were using\n",
    "    print ('Reading WIT metrics...')\n",
    "    wit_yearly = pd.read_csv(os.path.join(data_path,'RESULT_WIT_ANAE_yearly_metrics.csv')).rename(columns={'feature_id':'UID'})\n",
    "    #remove all records not on the managed floodplain\n",
    "    wit_yearly = wit_yearly[wit_yearly['UID'].isin(ANAE.index)]\n",
    "\n",
    "#debug code = limit to first 1000 records so it runs quickly    \n",
    "#wit_yearly = wit_yearly.head(1000)\n",
    "\n",
    "allyears = wit_yearly['year'].unique().tolist()  #list of all years included in the data frame\n",
    "baseline = [y for y in allyears if y not in millennium_drought]   #alltime excluding the millennium_drought\n",
    "\n",
    "\n",
    "\n",
    "# we pass a subset of metrics that we are interested in to the \"calc_pivots\" function\n",
    "#for the BWS project we calculated vegetation stress metrics in a 5year moving window looking at the trend\n",
    "# (rate and direction of change) within the most recent  the last 2 years\n",
    "#the summary pivot tables for each metric x aggregator combination are written to the working directory as CSV files\n",
    "\n",
    "metrics =  ['water+wet_median','pv_median', 'npv_median', 'npv+pv+wet_median']\n",
    "\n",
    "# water+wet_median represents inundation\n",
    "# pv_median =  fractional cover of green vegetation\n",
    "# npv =  fractional cover of non-green (brown) vegetation\n",
    "# npv+pv+wet_median = all vegetation (green, brown and wet vegetation)\n",
    "\n",
    "\n",
    "#npv+pv+wet =  combines all WIT vegetation, water+wet_median represents inundation\n",
    "calc_pivots (wit_yearly, metrics, year_window_width=veg_window_width, trend_window_width=veg_trend_width)\n",
    "\n",
    "#median bare soil (bs_median) is processed separately with the reverse_scores=True switch to reverse\n",
    "# because more bare soil represents declining condition\n",
    "\n",
    "metrics = ['bs_median'] \n",
    "calc_pivots (wit_yearly, metrics, year_window_width=veg_window_width, trend_window_width=veg_trend_width, reverse_scores=True) \n",
    "\n",
    "\n",
    "#waterbirds respond quicker than trees so the BWS project used annual metrics (year_window_width=1) and there is no multi-year trend\n",
    "metrics =  ['water+wet_median', 'pv_median']\n",
    "calc_pivots (wit_yearly, metrics, year_window_width=1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    metrics =  ['water+wet_median']\n",
    "    aggregators = {'Valley': Valley} \n",
    "    wit={}\n",
    "    year_window_width = 1\n",
    "    trend_window_width = 1\n",
    "    nobaseline = True\n",
    "    for metric in metrics:\n",
    "        metrics_df = deviation_from_baseline(wit_yearly, metric, year_window_width, trend_window_width, nobaseline = nobaseline)\n",
    "        print('Aggregate metrics and scores:')\n",
    "        for ag in aggregators:\n",
    "            fname = f\"{metric}_{ag}_{year_window_width}yr_INUNDATION.csv\"\n",
    "            print (f\"     {ag} - {fname}\")\n",
    "            wit[ag] = aggregate_area_weighted(metrics_df, aggregators[ag], aggfield[ag], ANAEgrp = ['grp']).round(4)\n",
    "            col_list = [c for c in wit[ag].columns.values.tolist() if c != 'Area_Ha']\n",
    "            #wit[ag] = append_metric_scores(wit[ag], col_list, _bins, _scores)\n",
    "            wit[ag].to_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics =  ['water+wet_max',]\n",
    "calc_pivots (wit_yearly, metrics, year_window_width=1, nobaseline=True) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wit_monthly = None\n",
    "\n",
    "#data_path = 'O:\\ANAE_WIT_Jan2024_RESULTS'\n",
    "\n",
    "if wit_monthly is None:   #read the data in if hasn't been read in already\n",
    "    #use geopandas instead of pandas so we can later call on the 'name' attribute whcih is not in the version of pandas were using\n",
    "    print ('Reading WIT metrics...')\n",
    "    wit_monthly = pd.read_csv(os.path.join(data_path,'RESULT_WIT_ANAE_monthly_metrics.csv')).rename(columns={'feature_id':'UID'})\n",
    "    #remove all records not on the managed floodplain\n",
    "    wit_monthly = wit_monthly[wit_monthly['UID'].isin(ANAE.index)]\n",
    "    \n",
    "    #convert year to water year\n",
    "    \n",
    "    wit_monthly.loc[wit_monthly['month'] > 6, 'year'] += 1\n",
    "\n",
    "\n",
    "#wit_monthly['wyear'] = df['base_date'].map(lambda d: d.year + 1 if d.month > 6 else d.year)\n",
    "\n",
    "wit_yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the WIT Time since last inundation\n",
    "Read in the WIT time since last inundation metrics and score the stress using the user defined thresholds for HIGH, MEDIUM and LOW stress that are defined in the code above (vegetation_tsli_score)\n",
    "\n",
    "**input:** WIT Time since last inundation statistics generated by the wit_metrics notebook as file **'RESULT_ANAE_time_since_last_inundation.csv**\n",
    "\n",
    "**output:** pivot tables for each spatial aggregator with the average time since last inundation in each calendar year for different vegetation groupings scored on a scale of 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tsli (df, name, thresholds):\n",
    "    '''\n",
    "        bins the time since last inundation metrics per polygon per year in df\n",
    "        into scores (1,2,3) appending the scores to the data frame\n",
    "        then aggregates the ANAE polygon score to the larger spatial scales\n",
    "    '''\n",
    "    #print(f\"score time since last inundation for {name}\")\n",
    "    for y in tqdm(alltime, desc = f\"Score time since last inundation for {name}\"):\n",
    "        df[f\"sc_tsli{y}\"] = df[['grp',f\"tsli{y}\"]].groupby('grp').apply(bin_stress_scores, (thresholds, f\"tsli{y}\")).reset_index(level= 0, drop= True) #bin_stress_scores is a function declared in this notebook\n",
    "    cols = [f\"tsli{y}\" for y in alltime]+[f\"sc_tsli{y}\" for y in alltime]\n",
    "    print('Aggregate metrics and scores:')\n",
    "    for ag in aggregators:\n",
    "        fname = f\"time_since_last_inundation_{ag}_{name}.csv\"\n",
    "        print (f\"     {ag} - {fname}\")\n",
    "        tsli_ag = aggregate_area_weighted(df[cols], aggregators[ag], aggfield[ag], ANAEgrp = ['grp'])\n",
    "        tsli_ag.round(1).to_csv(fname)\n",
    "\n",
    "\n",
    "\n",
    "if tsli_master is None:   #read the data in if required otherwise re-use\n",
    "    print ('Reading WIT time since last inundation...')\n",
    "    tsli_master = pd.read_csv(os.path.join(data_path,'RESULT_WIT_ANAE_time_since_last_inundation.csv')).rename(columns={'feature_id':'UID'}).set_index('UID')\n",
    "if idf_master is None:\n",
    "    print ('Reading WIT inundation metrics...')\n",
    "    idf_master = pd.read_csv(os.path.join(data_path,'RESULT_WIT_ANAE_inundation_metrics.csv'), parse_dates=['start_time', 'end_time']).rename(columns={'feature_id':'UID'})  #, converters = dict(duration=pd.to_timedelta, gap=pd.to_timedelta)\n",
    "    #much faster to convert these data types once the dataframe is loaded into pandas compared to using converters on csv read\n",
    "    idf_master['duration'] = pd.to_timedelta(idf_master['duration']).dt.days\n",
    "    idf_master['gap'] = pd.to_timedelta(idf_master['gap']).dt.days\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tsli_df=tsli_master.join(ANAE[['ANAE_TYPE', 'grp', 'Area_Ha']])\n",
    "\n",
    "\n",
    "for y in tqdm(alltime, desc = 'Time since last inundation per year'):\n",
    "    cutoff_date = np.datetime64(str(y)+'-12-31')\n",
    "\n",
    "    idf = idf_master[idf_master['start_time']<cutoff_date].copy()\n",
    "    idf.loc[idf['end_time']>cutoff_date, 'end_time'] = cutoff_date\n",
    "    idf.loc[idf['end_time']==cutoff_date, 'duration'] = (idf['end_time'] - idf['start_time'])\n",
    "\n",
    "    timesincelast = idf[(idf['end_time']==idf.groupby('UID')['end_time'].transform('max'))].copy().set_index('UID')\n",
    "    #timesincelast=timesincelast.join(tsli_df['final-date'].astype('datetime64[D]')) #causes and error in newer python  - use to_datetime instead\n",
    "    timesincelast=timesincelast.join(pd.to_datetime(tsli_df['final-date']))\n",
    "    timesincelast.loc[timesincelast['final-date'] > cutoff_date, 'final-date'] = cutoff_date\n",
    "    #tsli for this year\n",
    "    tsli_df[f\"tsli{y}\"] = (timesincelast['final-date'] - timesincelast['end_time']).dt.days\n",
    "    \n",
    "#time since last inundation of different vegetation functional groups scored by user defined thresholds\n",
    "    \n",
    "score_tsli(tsli_df, 'vegetation_stress', vegetation_tsli_stress_thresholds)\n",
    "\n",
    "score_tsli(tsli_df, 'waterbird_habitat_stress', waterbird_habitat_stress_thresholds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process NDVI\n",
    "Data obtained from two different Google Earth Engine data sets required to represent the full time period\n",
    "\n",
    "* 1986_1999 - NOAA AVHRR  https://developers.google.com/earth-engine/datasets/catalog/NOAA_CDR_AVHRR_NDVI_V5\n",
    "\n",
    "* 2000_2022 - MODIS https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13Q1\n",
    "\n",
    "\n",
    "\n",
    "The data sets are not directly comparable with NDVI values obtained from AVHRR being approx 50% of MODIS\n",
    "(a function of the data ranges as provided in Google's earth engine data library).\n",
    "\n",
    "Each data set is therefore standardised to range 0-1 before appending them\n",
    "\n",
    "**input:** average NDVI per ANAE polygon per year as CSV files **NDVI_1986_1999.csv** (NOAA AVHRR) and **NDVI_2000_2022.csv** (MODIS) where each file is the \n",
    "**output:** NDVI pivot tables for each spatial aggregation  as CSV files in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if ndvi is None:  #read the data in if required otherwise re-use\n",
    "    print ('Reading AVHRR_NDVI data...')\n",
    "    #NOAA_CDR_AVHRR_NDVI_V5 range -9998 to 9998\n",
    "    ndvi1 = pd.read_csv(os.path.join(data_path,'NDVI_1986_1999.csv'), dtype={'NDVI': float, 'UID': str, 'year': int})\n",
    "    ndvi1['NDVI'] = standardise(ndvi1['NDVI'])\n",
    "    \n",
    "    ndvi_z1 = ndvi1.copy()\n",
    "    \n",
    "    ndvi_z1['NDVI'] = standardise(ndvi1['NDVI'])\n",
    "    \n",
    "    #MODIS_061_MOD13Q1 range -9998 to 10000\n",
    "    print ('Reading MODIS_NDVI data...')\n",
    "    ndvi2 = pd.read_csv(os.path.join(data_path,'NDVI_2000_2024.csv'), dtype={'NDVI': float, 'UID': str, 'year': int})\n",
    "    ndvi2['NDVI'] = standardise(ndvi2['NDVI'])\n",
    "    ndvi_z2 = ndvi2.copy()\n",
    "    ndvi_z2['NDVI'] = standardise_z(ndvi1['NDVI'])\n",
    "    \n",
    "    ndvi = pd.concat([ndvi1, ndvi2], ignore_index=True)\n",
    "    ndvi_z = pd.concat([ndvi_z1, ndvi_z2], ignore_index=True)\n",
    "\n",
    "allyears = ndvi['year'].unique().tolist()  #list of all years included in the data frame\n",
    "baseline = [y for y in allyears if y not in millennium_drought]   #allyears excluding the millennium_drought\n",
    "\n",
    "# we pass a subset of metrics that we are interested in to the \"calc_pivots\" function\n",
    "#for the BWS project we calculated vegetation stress metrics in a 5year moving window looking at the trend\n",
    "# (rate and direction of change) within the most recent  the last 2 years\n",
    "#the summary pivot tables for each metric x aggregator combination are written to the working directory as CSV files\n",
    "\n",
    "metrics = ['NDVI']\n",
    "calc_pivots (ndvi, metrics, year_window_width=veg_window_width, trend_window_width=veg_trend_width)\n",
    "\n",
    "calc_pivots (ndvi_z, metrics, year_window_width=veg_window_width, trend_window_width=veg_trend_width, nobaseline=True, tag='_z')\n",
    "\n",
    "#waterbirds respond quicker than trees so the BWS project used annual metrics (year_window_width=1) and there is no multi-year trend\n",
    "calc_pivots (ndvi, metrics, year_window_width=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.polyfit(standardised_xvalues, yvalues, 1)[0].round(4)\n",
    "#regression AVHRR vs MODIS   https://stackoverflow.com/questions/48767977/simple-linear-regression-using-pandas-dataframe\n",
    "# df.groupby(\"SysNr\").apply(\n",
    "#     ...:     lambda g: np.polyfit(g.RegnskabsAar, g.res_f_r, 1)).apply(\n",
    "#     ...:     pd.Series).rename(columns={0:'slope', 1:'intercept'}).reset_index()\n",
    "\n",
    "\n",
    "AVHRR_NDVI = pd.read_csv(os.path.join(data_path,'AVHRR_NDVI_2000_2013.csv'), dtype={'NDVI': float, 'UID': str, 'year': int}).rename(columns={'NDVI':'NDVI_AVHRR'})\n",
    "MODIS_NDVI = ndvi2[ndvi2['year'] <= 2013].rename(columns={'NDVI':'NDVI_MODIS'})\n",
    "print(AVHRR_NDVI)\n",
    "print(MODIS_NDVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = AVHRR_NDVI.merge(MODIS_NDVI, on=['year', 'UID'])\n",
    "\n",
    "new_df = new_df[new_df['UID'].str.startswith('r1xhh')]\n",
    "from sklearn.metrics import r2_score\n",
    "def linear_regression(df):\n",
    "    #R_square = r2_score(xdf['NDVI_AVHRR'].to_numpy(), xdf['NDVI_MODIS'].to_numpy()) \n",
    "    return np.polyfit(df['NDVI_AVHRR'], df['NDVI_MODIS'], 1)\n",
    "\n",
    "new_df.groupby('UID').apply(linear_regression)\n",
    "\n",
    "\n",
    "\n",
    "#new_df[new_df['UID']=='r1x008j9z'].plot('NDVI_AVHRR', 'NDVI_MODIS', kind='scatter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zz=new_df[new_df['UID']=='r1xhh9zyq']\n",
    "print(zz)\n",
    "print(r2_score(, zz['NDVI_MODIS']))\n",
    "zz.plot('year', 'NDVI_AVHRR', kind='line')\n",
    "zz.plot('year', 'NDVI_MODIS', kind='line')\n",
    "zz.plot('NDVI_AVHRR', 'NDVI_MODIS', kind='scatter')\n",
    "zz.plot.scatter(x='NDVI_AVHRR', y='NDVI_MODIS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Soil Moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if soilmoisture_df is None:  #read the data in if required otherwise re-use\n",
    "    print ('Reading root zone soil moisture data...')\n",
    "    soilmoisture_df = pd.read_csv(os.path.join(data_path,'ZonalSt30_soilmoistureanomally.csv'))\n",
    "    soilmoisture_df['year'] = pd.to_datetime(soilmoisture_df['StdTime']).dt.year\n",
    "    soilmoisture_df = soilmoisture_df.rename(columns = {'MEAN':'soilmoist'})\n",
    "    \n",
    "#soil moisture is derived from the soil moisture anomaly data set - use different bins to set scoring _bins=[-1, 0.25, 0.5, 1]\n",
    "# since the metric is already expressed as an anomally we pass nobaseline = True to prevent calculation of baseline average\n",
    "                                  \n",
    "# we pass a subset of metrics that we are interested in to the \"calc_pivots\" function\n",
    "#for the BWS project we calculated vegetation stress metrics in a 5year moving window looking at the trend\n",
    "# (rate and direction of change) within the most recent  the last 2 years\n",
    "#the summary pivot tables for each metric x aggregator combination are written to the working directory as CSV files                                  \n",
    "                                  \n",
    "metrics = ['soilmoist']\n",
    "calc_pivots (soilmoisture_df, metrics, year_window_width=veg_window_width, trend_window_width=veg_trend_width, _bins=[-1, 0.25, 0.5, 1], reverse_scores=False, nobaseline=True) \n",
    "\n",
    "#waterbirds response is faster so windows width =1\n",
    "calc_pivots (soilmoisture_df, metrics, year_window_width=1, _bins=[-1, 0.25, 0.5, 1], reverse_scores=False, nobaseline=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all Vegetation Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "years = range(alltime[0] + veg_window_width - 1, alltime[-1] + 1)\n",
    "\n",
    "cond = ['meanTSC', 'npv+pv+wet_median', 'ndvi']\n",
    "stress = ['water+wet_median', 'time_since_last_inundation', 'SoilMoist']\n",
    "\n",
    "print (f\"\"\"\n",
    "Condition is the sum of {len(cond)} metrics, {cond}\n",
    "Stress is the sum of {len(stress)} metrics, {stress}\n",
    "\n",
    "The scores from saved pivot files in working_directory {working_directory}\"\n",
    "\n",
    "Summed scores for condition and stress are rescaled to range 0-1 to allow for missing data in spatial features\n",
    "\n",
    "Vulnerability = condition + stress (rescaled 0-1)\n",
    "\n",
    "A matrix with condition | stress | vulnerability scores is output for each aggregator scale...\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "#we process all aggregators so vulnerability can be assessed at any of the scales\n",
    "#for veg the BWS regions are the 'valleys'\n",
    "\n",
    "\n",
    "for ag in aggregators:\n",
    "    fname = f\"FINAL_BWSVulnerability_vegetation_{ag}.csv\"\n",
    "    print (f\"     {ag} - {fname}\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        *** CONDITION ***\n",
    "    '''\n",
    "    #load in the scores from the condition metric pivot tables that are contributing to the vulnerability score\n",
    "    #cond_a = extract_scores(ag, f\"meanTSC_{ag}_5yr.csv\", 'scsummeantsc')\n",
    "    cond_b = extract_scores(ag, f\"npv+pv+wet_median_{ag}_5yr.csv\", 'scsumnpvpvwet_median')\n",
    "    cond_c = extract_scores(ag, f\"ndvi_{ag}_5yr.csv\", 'scsumndvi')\n",
    "    \n",
    "    #stacking the three metrics vertically in a df with the same index allows us to use groupby sum and count by index feature\n",
    "   \n",
    "    combined_cond_df = pd.concat([cond_b, cond_c], axis=0)\n",
    "\n",
    "    # the condition score is the sum of multiple individual metric scores (nominally three for vegetation in this case)\n",
    "    # however can be sum of 2-3 metrics if there are missing data resulting in a lower sum.\n",
    "    # The simple normalise_data function would penailse cells with fewer metrics contributing (thus lower score)\n",
    "    # Therefore use a revised method that weights by the count of metrics contributing to each sum is used   \n",
    "    cond_df = sum_and_normalise_data_weighted(combined_cond_df, len(cond)).round(1)\n",
    "    \n",
    "    '''\n",
    "        *** STRESS ***\n",
    "    '''\n",
    "    #load in the scores from the stress metric pivot tables that are contributing to the vulnerability score\n",
    "    stress_a = extract_scores(ag, f\"water+wet_median_{ag}_5yr.csv\", 'scsumwaterwet_median')\n",
    "    stress_b = extract_scores(ag, f\"time_since_last_inundation_{ag}_vegetation_stress.csv\", 'sc_tsli')\n",
    "    stress_c = extract_scores(ag, f\"SoilMoist_{ag}_5yr.csv\", 'scsumsoilmoist')\n",
    "\n",
    "    #stacking the three metrics vertically in a df with the same index allows us to use groupby to sum and count by index feature\n",
    "    combined_stress_df = pd.concat([stress_a,stress_b,stress_c], axis=0)    \n",
    "\n",
    "    # the stress score is the sum of multiple individual metric scores (nominally three for vegetation in this case)\n",
    "    # however can be sum of 1-2 metrics if there are missing data resulting in a lower sum.\n",
    "    # The simple normalise_data function would penailse cells with fewer metrics contributing (thus lower score)\n",
    "    # Therefore use a revised method that weights by the count of metrics contributing to each sum is used   \n",
    "    stress_df = sum_and_normalise_data_weighted(combined_stress_df, len(stress)).round(1)\n",
    "    \n",
    "    '''\n",
    "        *** VULNERABILITY ***\n",
    "    '''\n",
    "    \n",
    "    #vulnerability is condition + stress normalised to range 0-1\n",
    "    tmp_df = pd.concat([cond_df, stress_df], axis=0)\n",
    "    vul_df = normalise_data(tmp_df.groupby(tmp_df.index).sum()).round(1)\n",
    "    \n",
    "    \n",
    "    #for compatibility with Excel for user data review having headers that are numbers (i.e. years) cause \"issues\"\n",
    "    #rename the columns from numerical years to strings with cond, stress, vul prefix so that Excel treats them as headers\n",
    "    \n",
    "    cond_df = cond_df.rename(columns = {c: f\"cond{c}\" for c in cond_df.columns})\n",
    "    stress_df = stress_df.rename(columns = {c: f\"stress{c}\" for c in stress_df.columns})\n",
    "    vul_df = vul_df.rename(columns = {c: f\"vul{c}\" for c in vul_df.columns})\n",
    "    \n",
    "    \n",
    "    #append columns for condition, stress and vulnerability scores into a single table\n",
    "    \n",
    "    score_df = pd.concat([cond_df, stress_df, vul_df], axis=1)\n",
    "    score_df.sort_index(axis=1).to_csv(fname)\n",
    "    \n",
    "    # NOTE output scale used for vegetaion in the report is Valley scale\n",
    "    # The relevant output file is FINAL_BWSVulnerability_vegetation_Valley.csv\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3-geopandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
